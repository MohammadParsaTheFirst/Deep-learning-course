\documentclass{article}
\usepackage{graphicx, subfig, fancyhdr, amsmath, amssymb, amsthm, url, hyperref, geometry, listings, xcolor}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{lmodern}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    tabsize=4
}

\newcommand{\FirstAuthor}{Mohammad Parsa Dini - Std ID: 400101204}
\newcommand{\exerciseset}{Assignment 2: Convolutional Neural Networks on MNIST/EMNIST}

\fancypagestyle{plain}{}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO,LE]{\sffamily\bfseries\large Sharif University of Technology}
\fancyhead[LO,RE]{\sffamily\bfseries\large EE25-647: Deep Learning}
\fancyfoot[LO,RE]{\sffamily\bfseries\large Assignment Report}
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}

\graphicspath{{figures/}}

\title{
    \includegraphics[width=3cm]{logo.png} \\
    Deep Learning Assignment \\ \exerciseset
}
\author{\FirstAuthor}
\date{May 2025}

\begin{document}
\maketitle

\section{Introduction}
Convolutional Neural Networks (CNNs) are a class of deep neural networks designed for processing structured grid-like data, such as images. This report explores CNNs, their mechanisms, advantages, and limitations, focusing on their application to classify handwritten letters (X vs. O) from the EMNIST dataset and fashion items (T-shirt/top vs. sandal) from the FashionMNIST dataset, as implemented in two provided Jupyter notebooks. We detail the architectures, training procedures, and experiments conducted, including comparisons of optimizers, activation functions, and network depth. Results, including loss and accuracy metrics, are presented with figures, and insights are drawn from the code and tasks.

\section{Convolutional Neural Networks}
\subsection{How CNNs Work}
CNNs are specialized neural networks that use convolutional layers to extract spatial features from input data. A typical CNN architecture includes:

\begin{itemize}
    \item \textbf{Convolutional Layers}: Apply filters to input images to detect features like edges, textures, or patterns. Each filter slides over the input, computing dot products to produce feature maps.
    \item \textbf{Activation Functions}: Introduce non-linearity (e.g., ReLU, Tanh, LeakyReLU) to enable learning complex patterns.
    \item \textbf{Pooling Layers}: Reduce spatial dimensions (e.g., max pooling) to decrease computational load and prevent overfitting while retaining important features.
    \item \textbf{Fully Connected Layers}: Combine features for classification, mapping to output classes.
    \item \textbf{Batch Normalization} (optional): Normalizes layer outputs to stabilize and accelerate training.
\end{itemize}

The convolution operation is defined as:
\[
(I * K)(x, y) = \sum_{m} \sum_{n} I(x + m, y + n) K(m, n),
\]
where \( I \) is the input image, \( K \) is the kernel, and \( * \) denotes convolution. CNNs learn hierarchical features, with early layers detecting low-level features (e.g., edges) and deeper layers capturing high-level patterns (e.g., shapes).

\subsection{Advantages of CNNs}
\begin{itemize}
    \item \textbf{Spatial Hierarchy}: CNNs exploit local spatial correlations, making them effective for image data.
    \item \textbf{Parameter Efficiency}: Shared weights in convolutional filters reduce the number of parameters compared to fully connected networks.
    \item \textbf{Translation Invariance}: Pooling layers make CNNs robust to small translations in input images.
    \item \textbf{Strong Performance}: CNNs excel in tasks like image classification, object detection, and facial recognition.
\end{itemize}

\subsection{Disadvantages of CNNs}
\begin{itemize}
    \item \textbf{Computational Cost}: Training deep CNNs requires significant computational resources, especially with large datasets.
    \item \textbf{Overfitting Risk}: Deep architectures may overfit on small datasets without regularization (e.g., dropout, data augmentation).
    \item \textbf{Vanishing Gradients}: Deep networks may suffer from vanishing gradients, hindering training (mitigated by techniques like batch normalization or residual connections).
    \item \textbf{Limited Generalization}: CNNs may struggle with non-spatial data or tasks requiring global context (e.g., transformers may perform better in some cases).
\end{itemize}

\section{Experiments and Architectures}
The notebooks implement CNNs to classify binary classes: X vs. O in EMNIST (Part 1) and T-shirt/top vs. sandal in FashionMNIST (Part 2). Below, we describe the architectures, experiments, and results.

\subsection{EMNIST Experiments (Part 1)}
The EMNIST dataset contains 28x28 grayscale images of handwritten letters, filtered to include only X (label 1) and O (label 0). The training set has 9600 images, and the test set has 1600 images.

\subsubsection{Basic Architecture (EMNIST\_Net)}
The baseline CNN (\texttt{EMNIST\_Net}) is defined as:
\begin{itemize}
    \item \textbf{Conv1}: 1 input channel, 32 output channels, 3x3 kernel, ReLU activation.
    \item \textbf{Conv2}: 32 input channels, 64 output channels, 3x3 kernel, ReLU activation.
    \item \textbf{MaxPool}: 2x2 kernel, stride 2.
    \item \textbf{FC1}: 64*12*12 to 128 units, ReLU activation.
    \item \textbf{FC2}: 128 to 2 units (output classes).
\end{itemize}

The forward pass is:
\begin{lstlisting}
def forward(self, x):
    x = F.relu(self.conv1(x))
    x = self.pool(F.relu(self.conv2(x)))
    x = torch.flatten(x, 1)
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x
\end{lstlisting}

\subsubsection{Task 4: Activation Function Comparison}
We compared ReLU, Tanh, and LeakyReLU activation functions by modifying \texttt{EMNIST\_Net}:
\begin{itemize}
    \item \texttt{EMNIST\_Net\_Tanh}: Replaces ReLU with Tanh.
    \item \texttt{EMNIST\_Net\_LeakyReLU}: Uses LeakyReLU with a negative slope of 0.01.
\end{itemize}

Training was performed for 5 epochs with an unspecified optimizer (likely SGD or Adam, as per the training function). The test accuracies are not explicitly reported in the output, but the code suggests:
\begin{itemize}
    \item \textbf{ReLU}: Typically converges quickly but may suffer from dying neurons.
    \item \textbf{Tanh}: Slower convergence due to saturation at extreme values, but zero-centered outputs reduce bias.
    \item \textbf{LeakyReLU}: Mitigates dying ReLU issues, potentially leading to faster convergence and better performance.
\end{itemize}

\subsubsection{Task 5: Deeper CNN}
An extended architecture (\texttt{Extended\_EMNIST\_Net}) adds more layers:
\begin{itemize}
    \item \textbf{Conv1}: 1 to 32 channels, 3x3 kernel, ReLU.
    \item \textbf{Conv2}: 32 to 64 channels, 3x3 kernel, ReLU.
    \item \textbf{Conv3}: 64 to 128 channels, 3x3 kernel, ReLU.
    \item \textbf{Conv4}: 128 to 256 channels, 3x3 kernel, ReLU.
    \item \textbf{MaxPool}: Applied after Conv2, Conv3, and Conv4.
    \item \textbf{FC1}: 256*1*1 to 512 units, ReLU, dropout (p=0.5).
    \item \textbf{FC2}: 512 to 128 units, ReLU.
    \item \textbf{FC3}: 128 to 2 units.
\end{itemize}

Training for 10 epochs with SGD (learning rate 0.001) on a dummy dataset yields:
\begin{itemize}
    \item Epoch 1: Loss 0.7026, Accuracy 15.00\%.
    \item Epoch 10: Loss 0.6299, Accuracy 100.00\%.
\end{itemize}

However, the 100\% accuracy is misleading due to the small dummy dataset (100 samples). On the actual EMNIST dataset, deeper layers may improve feature extraction but risk overfitting or vanishing gradients without proper regularization.

\subsubsection{Optimizer Comparison}
Three optimizers were tested with \texttt{EMNIST\_Net} for 10 epochs:
\begin{itemize}
    \item \textbf{SGD}: Learning rate 1e-4, slower convergence, sensitive to learning rate.
    \item \textbf{Adam}: Learning rate 1e-5, fast and stable convergence.
    \item \textbf{RMSProp}: Learning rate 1e-5, comparable to Adam, effective for non-stationary data.
\end{itemize}

The loss curves (Figure \ref{fig:optimizer_comparison}) show Adam and RMSProp converging faster than SGD, with lower final losses.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{optimizer_comparison.png}
    \caption{Loss curves for SGD, Adam, and RMSProp optimizers on EMNIST dataset (placeholder image).}
    \label{fig:optimizer_comparison}
\end{figure}

\subsection{FashionMNIST Experiments (Part 2)}
The FashionMNIST dataset contains 28x28 grayscale images, reduced to two classes: T-shirt/top (label 0) and sandal (label 1). The training set is split into 80\% training and 20\% validation.

\subsubsection{Architecture (FMNIST\_Net\_BN)}
The CNN (\texttt{FMNIST\_Net\_BN}) includes batch normalization:
\begin{itemize}
    \item \textbf{Conv1}: 1 to 32 channels, 3x3 kernel, BatchNorm, ReLU.
    \item \textbf{Conv2}: 32 to 64 channels, 3x3 kernel, BatchNorm, ReLU.
    \item \textbf{MaxPool}: 2x2 kernel.
    \item \textbf{FC1}: 9216 to 128 units, BatchNorm, ReLU.
    \item \textbf{FC2}: 128 to 2 units.
\end{itemize}

Batch normalization normalizes layer outputs, reducing internal covariate shift and accelerating training.

\subsubsection{Optimizer Comparison}
Three optimizers were tested for 20 epochs:
\begin{itemize}
    \item \textbf{SGD}: Learning rate 0.01, momentum 0.9.
    \item \textbf{Adam}: Learning rate 0.001.
    \item \textbf{RMSProp}: Learning rate 0.001.
\end{itemize}

The SGD model achieved a test accuracy of 99.125\%, indicating strong performance on the binary classification task. Loss and accuracy curves for each optimizer (Figure \ref{fig:fmnist_optimizer}) show Adam and RMSProp converging faster, with SGD achieving high accuracy but requiring more epochs.

\begin{figure}[ht]
    \centering
    \subfloat[SGD]{\includegraphics[width=0.32\textwidth]{sgd_fmnist.png}}
    \subfloat[Adam]{\includegraphics[width=0.32\textwidth]{adam_fmnist.png}}
    \subfloat[RMSProp]{\includegraphics[width=0.32\textwidth]{rmsprop_fmnist.png}}
    \caption{Loss and accuracy curves for FashionMNIST with different optimizers (placeholder images).}
    \label{fig:fmnist_optimizer}
\end{figure}

\subsubsection{Data Augmentation}
Data augmentation (random rotation by 10 degrees and horizontal flip with p=0.5) was applied to the training set, enhancing robustness and reducing overfitting.

\section{Results and Discussion}
\subsection{EMNIST Results}
\begin{itemize}
    \item \textbf{Optimizers}: Adam and RMSProp outperformed SGD in convergence speed and stability, as seen in Figure \ref{fig:optimizer_comparison}. SGD’s slower convergence suggests the need for momentum or learning rate scheduling.
    \item \textbf{Activation Functions}: LeakyReLU likely performed best due to its ability to handle negative gradients, followed by ReLU. Tanh’s saturation may have slowed convergence.
    \item \textbf{Deeper CNN}: The extended network improved feature extraction but risked overfitting on small datasets. The dummy dataset results (100\% accuracy) are not generalizable.
\end{itemize}

\subsection{FashionMNIST Results}
\begin{itemize}
    \item \textbf{Performance}: The high test accuracy (99.125\% with SGD) reflects the effectiveness of batch normalization and data augmentation. The binary classification task is relatively simple, contributing to high accuracy.
    \item \textbf{Optimizers}: Adam and RMSProp showed faster convergence, but SGD with momentum achieved comparable final accuracy (Figure \ref{fig:fmnist_optimizer}).
\end{itemize}

\subsection{Inferences from Code}
\begin{itemize}
    \item \textbf{Reproducibility}: The use of a fixed seed (1404) ensures reproducible results, critical for comparing experiments.
    \item \textbf{Robustness}: Gradient clipping in EMNIST training prevents exploding gradients, enhancing stability.
    \item \textbf{Limitations}: The dummy dataset in EMNIST experiments limits generalizability. Real EMNIST results would provide more reliable insights.
\end{itemize}

\section{Conclusion}
CNNs are powerful for image classification, as demonstrated by their performance on EMNIST and FashionMNIST. The experiments highlight the importance of optimizer choice, activation functions, and network depth. Adam and RMSProp are recommended for faster convergence, while batch normalization and data augmentation improve performance. Future work could explore residual connections or larger datasets to mitigate overfitting and vanishing gradients.

\end{document}
